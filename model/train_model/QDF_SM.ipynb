{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1d9758",
   "metadata": {},
   "source": [
    "# Entraînement du modèle Quantum Deep Field (QDF) pour les SM\n",
    "\n",
    "Ce notebook contient le code pour entraîner un modèle QDF afin de prédire les propriétés des SM (Small molecule). Il comprend la définition du modèle, les classes pour l'entraînement et le test, ainsi que la gestion des données et l'optimisation des hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33303172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from e3nn.o3 import spherical_harmonics\n",
    "import torch.utils.data\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\" # pour deboguer les erreurs CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f3737",
   "metadata": {},
   "source": [
    "## Définition du modèle Quantum Deep Field (QDF)\n",
    "\n",
    "La classe `QuantumDeepField` définit l'architecture du réseau de neurones. Elle comprend :\n",
    "- Des couches d'embedding pour les coefficients et les exposants zêta des orbitales atomiques.\n",
    "- Des couches linéaires pour le fonctionnel de densité (prédiction des propriétés).\n",
    "- Des couches linéaires pour la carte de Hohenberg-Kohn (contraintes de densité).\n",
    "- Des fonctions pour construire la matrice de base (combinaison d'harmoniques sphériques et de fonctions radiales gaussiennes).\n",
    "- La fonction `LCAO` (Linear Combination of Atomic Orbitals) pour calculer les orbitales moléculaires.\n",
    "- La fonction `functional` pour appliquer le fonctionnel de densité appris.\n",
    "- La fonction `HKmap` pour appliquer la carte de Hohenberg-Kohn.\n",
    "- La fonction `forward` qui définit l'evaluation de la perte du modèle pour l'entraînement, le test et la prédiction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd73738",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumDeepField(nn.Module):\n",
    "    \n",
    "    def __init__(self, device, N_orbitals, dim, layer_functional, operation, N_output, hidden_HK, layer_HK):\n",
    "        super(QuantumDeepField, self).__init__()    \n",
    "        \n",
    "        self.coefficient = nn.Embedding(N_orbitals, dim) # embedings de chaque orbitale avec dim comme dimension\n",
    "        self.zeta = nn.Embedding(N_orbitals, 1) \n",
    "        nn.init.ones_(self.zeta.weight) \n",
    "        \n",
    "        self.W_functional = nn.ModuleList([nn.Linear(dim, dim) for i in range(layer_functional)]) # layer pour prédire les propriétés\n",
    "        self.bn_functional = nn.ModuleList([nn.BatchNorm1d(dim) for i in range(layer_functional)])\n",
    "        self.dropout_functional = nn.ModuleList([nn.Dropout(p=0.15) for i in range(layer_functional)])\n",
    "\n",
    "        self.W_property = nn.Linear(dim, N_output)\n",
    "        \n",
    "        \n",
    "        self.W_density = nn.Linear(1, hidden_HK) # layer pour les contraintes de HK ayant comme entrée la densité\n",
    "        self.W_HK = nn.ModuleList([nn.Linear(hidden_HK, hidden_HK) for i in range(layer_HK)])\n",
    "        self.W_potential = nn.Linear(hidden_HK, 1)\n",
    "        \n",
    "        self.device = device # device = 'cuda' or 'cpu'\n",
    "        self.dim = dim\n",
    "        self.layer_functional = layer_functional\n",
    "        self.operation = operation  # operation = 'sum' or 'concatenate'\n",
    "        self.layer_HK = layer_HK\n",
    "        \n",
    "        self.prelu = nn.PReLU()\n",
    "    \n",
    "    def list_to_batch(self, xs, dtype=torch.FloatTensor, cat=None, axis=None):\n",
    "        \"\"\"Transforme une liste de données numpy en un batch de tenseurs PyTorch.\"\"\"\n",
    "        xs = [dtype(x).to(self.device) for x in xs]\n",
    "        if cat:\n",
    "            return torch.cat(xs, axis)\n",
    "        else:\n",
    "            return xs  # w/o cat (i.e., the list (not batch) of tensor data).\n",
    "        \n",
    "    def pad(self, matrices, pad_value):\n",
    "        \"\"\"Ajoute du padding à une liste de matrices\n",
    "        avec une valeur de padding (ex: 0 ou une grande valeur) pour le traitement par batch.\n",
    "        Par exemple, pour une liste de matrices [A, B, C],\n",
    "        cette fonction retourne une nouvelle matrice [A00, 0B0, 00C],\n",
    "        où 0 est la matrice nulle (i.e., une matrice diagonale par blocs).\n",
    "        \"\"\"\n",
    "        shapes = [m.shape for m in matrices]\n",
    "        M, N = sum([s[0] for s in shapes]), sum([s[1] for s in shapes])\n",
    "        pad_matrices = torch.full((M, N), pad_value, device=self.device)\n",
    "        i, j = 0, 0\n",
    "        for k, matrix in enumerate(matrices):\n",
    "            matrix = torch.FloatTensor(matrix).to(self.device)\n",
    "            m, n = shapes[k]\n",
    "            pad_matrices[i:i+m, j:j+n] = matrix\n",
    "            i += m\n",
    "            j += n\n",
    "        return pad_matrices\n",
    "    \n",
    "    \n",
    "    def basis_matrix(self, atomic_orbitals, distance_matrices, quantum_numbers, fields_coords):\n",
    "        \"\"\"Calcule la matrice de base en combinant les fonctions radiales (GTOs) et angulaires (harmoniques sphériques).\"\"\"\n",
    "        n = quantum_numbers\n",
    "        l = n - 1 # Le nombre quantique azimutal l est dérivé de n\n",
    "        \n",
    "        l = l.detach().cpu().tolist()\n",
    "        \n",
    "        l = [int(x) for x in l]\n",
    "        \n",
    "        # Calcule les harmoniques sphériques Y_lm pour les coordonnées des champs (points dans l'espace)\n",
    "        Y_lm = spherical_harmonics(l, fields_coords, normalize='True')\n",
    "        \n",
    "        # Assure la cohérence des dimensions pour la multiplication matricielle\n",
    "        Y_lm = Y_lm[:, :fields_coords.shape[0]] \n",
    "        \n",
    "        \n",
    "        # Récupère les exposants zêta pour chaque orbitale atomique\n",
    "        zetas = torch.squeeze(self.zeta(atomic_orbitals))\n",
    "        \n",
    "        # Calcule la partie radiale de la base (type GTO modifié)\n",
    "        Radials = (distance_matrices**(quantum_numbers-1) * torch.exp(-zetas*distance_matrices**2))\n",
    "        # Normalise les fonctions radiales\n",
    "        Radials = F.normalize(Radials, p=2, dim=0)    \n",
    "        \n",
    "        # print(torch.sum(torch.t(Radials)[0]**2))  # Vérification de la normalisation\n",
    "        \n",
    "        # print(\"Radials\", Radials.shape, \"Y_lm\", Y_lm.shape) # Vérification des dimensions\n",
    "\n",
    "        # Combine les parties radiales et angulaires pour obtenir la matrice de base\n",
    "        GTOs = torch.matmul(Radials, Y_lm)\n",
    "        \n",
    "        return GTOs\n",
    "    \n",
    "    \n",
    "    def LCAO(self, inputs):\n",
    "        \"\"\"Calcule les orbitales moléculaires par Combinaison Linéaire d'Orbitales Atomiques (LCAO).\"\"\"\n",
    "        (atomic_orbitals, distance_matrices,\n",
    "         quantum_numbers, atomic_coords, N_electrons, N_fields) = inputs\n",
    "\n",
    "        \"\"\"Prépare les données d'entrée pour le traitement par batch (concaténation ou padding).\"\"\"\n",
    "        atomic_orbitals = self.list_to_batch(atomic_orbitals, torch.LongTensor)\n",
    "        distance_matrices = self.pad(distance_matrices, 1e6)\n",
    "        quantum_numbers = self.list_to_batch(quantum_numbers, cat=True, axis=1)\n",
    "        N_electrons = self.list_to_batch(N_electrons)\n",
    "        atomic_coords = self.list_to_batch(atomic_coords, cat=True, axis=0)\n",
    "\n",
    "        \n",
    "        \"\"\"Normalise les coefficients (embeddings) des orbitales atomiques.\"\"\"\n",
    "        coefficients = []\n",
    "        for AOs in atomic_orbitals:\n",
    "            coefs = F.normalize(self.coefficient(AOs), 2, 0)\n",
    "            #print(torch.sum(torch.t(coefs)[0]**2))  # Vérification de la normalisation.\n",
    "            coefficients.append(coefs)\n",
    "        coefficients = torch.cat(coefficients)  # Concatène les coefficients pour le batch\n",
    "        atomic_orbitals = torch.cat(atomic_orbitals)\n",
    "        \n",
    "\n",
    "        \"\"\"Calcul LCAO.\"\"\"\n",
    "        \n",
    "        quantum_numbers = quantum_numbers[0] # Prend les nombres quantiques du premier élément (supposés identiques dans le batch)\n",
    "        \n",
    "        # Calcule la matrice de base\n",
    "        basis_matrix = self.basis_matrix(atomic_orbitals,\n",
    "                                        distance_matrices, quantum_numbers, atomic_coords)\n",
    "        #print(\"basis_matrix\", basis_matrix.shape, \"coefficients\", coefficients.shape)\n",
    "        # Calcule les orbitales moléculaires par produit matriciel\n",
    "        molecular_orbitals = torch.matmul(basis_matrix, coefficients)\n",
    "        \n",
    "\n",
    "        \"\"\"Normalise les orbitales moléculaires pour conserver le nombre total d'électrons.\"\"\"\n",
    "            \n",
    "        # Sépare les orbitales moléculaires par molécule dans le batch\n",
    "        split_MOs = torch.split(molecular_orbitals, N_fields)   # a reverifier si erreur\n",
    "        normalized_MOs = []\n",
    "        for N_elec, MOs in zip(N_electrons, split_MOs):\n",
    "            # Normalise chaque ensemble d'orbitales moléculaires\n",
    "            MOs = torch.sqrt(N_elec/self.dim) * F.normalize(MOs, 2, 0)\n",
    "            # print(torch.sum(MOs**2), N_elec)  # Vérification du nombre total d'électrons.\n",
    "            normalized_MOs.append(MOs)\n",
    "\n",
    "        # Concatène les orbitales moléculaires normalisées pour le batch\n",
    "        return torch.cat(normalized_MOs)\n",
    "    \n",
    "    \n",
    "    def functional(self, vectors, layers, operation, axis):\n",
    "        \"\"\"Applique le fonctionnel de densité basé sur un réseau de neurones profond (DNN).\"\"\"\n",
    "        for l in range(layers):\n",
    "            input_vectors = vectors\n",
    "            # Couche linéaire\n",
    "            out_vectors = self.W_functional[l](input_vectors)\n",
    "            # Normalisation par batch\n",
    "            out_vectors = self.bn_functional[l](out_vectors)        \n",
    "            # Fonction d'activation PReLU\n",
    "            vectors = self.prelu(out_vectors)  # Autres fonctions d'activation possible à tester (relu, leaky_relu, selu, silu, mish, gelu, tanh)\n",
    "            # Couche Dropout pour la régularisation\n",
    "            out_vectors = self.dropout_functional[l](out_vectors) \n",
    "            # Connexion résiduelle si les dimensions correspondent\n",
    "            if out_vectors.shape == input_vectors.shape:\n",
    "                vectors = out_vectors + input_vectors\n",
    "\n",
    "        # Opération finale pour agréger les vecteurs (somme ou moyenne)\n",
    "        if operation == 'sum':  # pour propriétés des matériaux comme PCE\n",
    "            vectors = [torch.sum(vs, 0) for vs in torch.split(vectors, axis)]\n",
    "        if operation == 'mean':  # pour Homo ou Lumo\n",
    "            vectors = [torch.mean(vs, 0) for vs in torch.split(vectors, axis)]\n",
    "        # Empile les vecteurs résultants pour former un tenseur\n",
    "        return torch.stack(vectors)\n",
    "\n",
    "    def HKmap(self, scalars, layers):\n",
    "        \"\"\"Applique la carte de Hohenberg-Kohn basée sur un DNN pour prédire le potentiel à partir de la densité.\"\"\"\n",
    "        # Première couche linéaire prenant la densité en entrée\n",
    "        vectors = self.W_density(scalars)\n",
    "        # Couches cachées avec activation ReLU\n",
    "        for l in range(layers):\n",
    "            vectors = torch.relu(self.W_HK[l](vectors))\n",
    "        # Couche de sortie pour prédire le potentiel\n",
    "        return self.W_potential(vectors)\n",
    "    \n",
    "    def forward(self, data, train=False, target=None, predict=False):\n",
    "        \"\"\"Ecaluation du modèle QDF.\"\"\"\n",
    "\n",
    "        # Extrait les données d'entrée\n",
    "        idx, inputs, N_fields = data[0], data[1:7], data[6]\n",
    "        \n",
    "        if predict: # Mode prédiction (pas de calcul de gradient)\n",
    "            with torch.no_grad():\n",
    "                molecular_orbitals = self.LCAO(inputs)\n",
    "                final_layer = self.functional(molecular_orbitals,self.layer_functional,self.operation, N_fields)\n",
    "                PCE_ = self.W_property(final_layer) # Prédiction de la propriété (PCE)\n",
    "                return idx, PCE_\n",
    "            \n",
    "        elif train: # Mode entraînement\n",
    "            if target == 'PCE':  # Apprentissage supervisé pour l'énergie (PCE)\n",
    "                PCE = self.list_to_batch(data[7], cat=True, axis=0)  # PCE réel\n",
    "                molecular_orbitals = self.LCAO(inputs)\n",
    "                final_layer = self.functional(molecular_orbitals,self.layer_functional,self.operation, N_fields)\n",
    "                PCE_ = self.W_property(final_layer)  # PCE prédit\n",
    "                #print(\"LOSS_PCE\", F.l1_loss(PCE, PCE_))\n",
    "                loss = F.l1_loss(PCE, PCE_) # Calcul de la perte L1 (MAE), on pourrait utiliser une perte RMSE aussi mais il faudrait réécrire la fonction de perte de la class Test et trainer\n",
    "            if target == 'V':  # Apprentissage supervisé pour le potentiel (via HK map)\n",
    "                V = self.list_to_batch(data[8], cat=True, axis=0)  # Potentiel réel (si disponible)\n",
    "                molecular_orbitals = self.LCAO(inputs)\n",
    "                # Calcule la densité électronique à partir des orbitales moléculaires\n",
    "                densities = torch.sum(molecular_orbitals**2, 1)\n",
    "                densities = torch.unsqueeze(densities, 1)\n",
    "                V_ = self.HKmap(densities, self.layer_HK)  # Potentiel prédit par la carte HK\n",
    "                loss = F.l1_loss(V, V_) # Calcul de la perte L1 (MAE) entre potentiel prédit et réel\n",
    "            return loss\n",
    "        \n",
    "        else:  # Mode test (évaluation, pas de calcul de gradient)\n",
    "            with torch.no_grad():\n",
    "                PCE = self.list_to_batch(data[7], cat=True, axis=0) # PCE réel\n",
    "                molecular_orbitals = self.LCAO(inputs)\n",
    "                final_layer = self.functional(molecular_orbitals,self.layer_functional,self.operation, N_fields)\n",
    "                PCE_ = self.W_property(final_layer)  # PCE prédit\n",
    "                PCE_ = PCE_.to(self.device)\n",
    "                return idx, PCE, PCE_ # Retourne ID, PCE réel, PCE prédit\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbead56",
   "metadata": {},
   "source": [
    "## Classe Trainer\n",
    "\n",
    "La classe `Trainer` gère le processus d'entraînement du modèle. Elle initialise l'optimiseur (Adam) et un planificateur de taux d'apprentissage (StepLR). La méthode `train` effectue une époque d'entraînement en calculant et en optimisant deux fonctions de perte :\n",
    "1.  **Perte PCE (Supervisée)** : Basée sur l'erreur absolue moyenne (L1 Loss) entre le PCE prédit et le PCE réel.\n",
    "2.  **Perte V (Supervisée)** : Basée sur l'erreur absolue moyenne (L1 Loss) entre le potentiel prédit par la carte HK et le potentiel réel ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabe8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, model, lr, lr_decay, step_size):\n",
    "        self.model = model\n",
    "        # Initialise l'optimiseur Adam avec le taux d'apprentissage donné\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr)\n",
    "        # Initialise un planificateur pour réduire le taux d'apprentissage toutes les 'step_size' époques\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size, lr_decay)  \n",
    "        \n",
    "    def optimize(self, loss, optimizer):\n",
    "        \"\"\"Effectue une étape d'optimisation (rétropropagation et mise à jour des poids).\"\"\"\n",
    "        optimizer.zero_grad() # Remet à zéro les gradients accumulés\n",
    "        loss.backward() # Calcule les gradients par rétropropagation\n",
    "        optimizer.step()  # Met à jour les paramètres du modèle\n",
    "        \n",
    "    def train(self, dataloader):\n",
    "        \"\"\"Effectue une époque d'entraînement complète sur le dataloader fourni.\"\"\"\n",
    "        losses_PCE, losses_V = 0, 0 # Initialise les pertes cumulées\n",
    "        # Calcule le nombre total d'échantillons dans le dataloader\n",
    "        n_batches = sum([len(data[0]) for data in dataloader])\n",
    "        for data in dataloader:\n",
    "            # Calcule la perte pour la cible 'PCE' (supervisé)\n",
    "            loss_PCE = self.model.forward(data, train=True, target='PCE')\n",
    "            # Optimise le modèle en fonction de la perte PCE\n",
    "            self.optimize(loss_PCE, self.optimizer)\n",
    "            losses_PCE += loss_PCE.item() # Accumule la perte PCE\n",
    "            # Calcule la perte pour la cible 'V' (non supervisé via HK map)\n",
    "            loss_V = self.model.forward(data, train=True, target='V')\n",
    "            # Optimise le modèle en fonction de la perte V\n",
    "            self.optimize(loss_V, self.optimizer)\n",
    "            losses_V += loss_V.item() # Accumule la perte V\n",
    "            #print(\"Loss V\", loss_V)\n",
    "        self.scheduler.step()\n",
    "        #print(\"n_bacthes\", n_batches)\n",
    "        # Retourne les pertes moyennes pour l'époque\n",
    "        return losses_PCE/n_batches, losses_V/n_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a207a3",
   "metadata": {},
   "source": [
    "## Classe Tester\n",
    "\n",
    "La classe `Tester` est responsable de l'évaluation du modèle sur un jeu de données (généralement validation ou test). La méthode `test` calcule l'erreur absolue moyenne (MAE) entre les prédictions et les valeurs réelles et génère une chaîne de caractères formatée contenant les identifiants, les valeurs correctes, les prédictions et l'erreur pour chaque échantillon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958f4eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Tester(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def accuracy(self, PCEs, PCEs_, seuil=2.8):\n",
    "        \"\"\"Proportion de prédictions avec erreur absolue < seuil.\"\"\"\n",
    "        PCEs = np.array(PCEs)\n",
    "        PCEs_ = np.array(PCEs_)\n",
    "        correct = np.abs(PCEs - PCEs_) < seuil\n",
    "        return np.mean(correct)\n",
    "\n",
    "    def test(self, dataloader, time=False):\n",
    "        \"\"\"Évalue le modèle sur le dataloader fourni et calcule la MAE.\"\"\"\n",
    "        # Calcule le nombre total d'échantillons\n",
    "        N = sum([len(data[0]) for data in dataloader])\n",
    "        IDs, PCEs, PCEs_ = [], [], [] # Listes pour stocker les IDs, valeurs réelles et prédites\n",
    "        SAE = 0 \n",
    "        start = timeit.default_timer() # Démarre le chronomètre\n",
    "        \n",
    "        for i, data in enumerate(dataloader):\n",
    "            # Effectue la prédiction en mode test (pas de calcul de gradient)\n",
    "            idx, PCE, PCE_ = self.model.forward(data)\n",
    "            # Calcule l'erreur absolue pour le batch\n",
    "            SAE_batch = torch.sum(torch.abs(PCE - PCE_), 0)\n",
    "            SAE += SAE_batch # Accumule l'erreur absolue\n",
    "            IDs += list(idx) # Ajoute les IDs du batch\n",
    "            PCEs += PCE.tolist() # Ajoute les valeurs réelles du batch\n",
    "            PCEs_ += PCE_.tolist() # Ajoute les valeurs prédites du batch\n",
    "\n",
    "            # Estime le temps restant (si time=True)\n",
    "            if (time is True and i == 0):\n",
    "                time_elapsed = timeit.default_timer() - start\n",
    "                minutes = len(dataloader) * time_elapsed / 60\n",
    "                hours = int(minutes / 60)\n",
    "                minutes = int(minutes - 60 * hours)\n",
    "                print('La prédiction se terminera dans environ',\n",
    "                      hours, 'heures', minutes, 'minutes.')\n",
    "\n",
    "        # Calcule l'erreur absolue moyenne (MAE)\n",
    "        MAE = (SAE/N).tolist()  \n",
    "        # Formate la MAE en chaîne de caractères (utile si plusieurs sorties)\n",
    "        MAE = ','.join([str(m) for m in MAE])  \n",
    "        \n",
    "        accuracy = self.accuracy(PCEs, PCEs_, seuil=2.8) # Calcule la précision\n",
    "        \n",
    "        # Crée une chaîne de caractères pour stocker les résultats détaillés\n",
    "        prediction = 'ID\\tCorrect\\tPredict\\tError\\n'\n",
    "        for idx, PCE, PCE_ in zip(IDs, PCEs, PCEs_):\n",
    "            # Calcule l'erreur absolue pour chaque échantillon\n",
    "            error = np.abs(np.array(PCE) - np.array(PCE_))\n",
    "            error = ','.join([str(e) for e in error]) # Formate l'erreur\n",
    "            # Formate les valeurs PCE et PCE_\n",
    "            PCE = str(PCE[0])\n",
    "            PCE_ = str(PCE_[0])\n",
    "            # Ajoute la ligne au résultat\n",
    "            prediction += '\\t'.join([idx, PCE, PCE_, error]) + '\\n'\n",
    "\n",
    "        return MAE, prediction, accuracy\n",
    "    \n",
    "    def save_result(self, result, filename):\n",
    "        \"\"\"Sauvegarde un résultat (ex: MAE) dans un fichier texte.\"\"\"\n",
    "        with open(filename, 'a') as f:\n",
    "            f.write(result + '\\n')\n",
    "\n",
    "    def save_prediction(self, prediction, filename):\n",
    "        \"\"\"Sauvegarde la chaîne de prédiction détaillée dans un fichier texte.\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(prediction)\n",
    "\n",
    "    def save_model(self, model, filename):\n",
    "        \"\"\"Sauvegarde les poids du modèle entraîné.\"\"\"\n",
    "        torch.save(model.state_dict(), filename)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525adc82",
   "metadata": {},
   "source": [
    "## Gestion des Données\n",
    "\n",
    "- **Classe `MyDataset`**: Hérite de `torch.utils.data.Dataset` et charge les fichiers de données pré-traitées (fichiers `.npy`) à partir d'un répertoire spécifié.\n",
    "- **Fonction `mydataloader`**: Crée un `DataLoader` PyTorch à partir d'un `MyDataset`. Elle gère le traitement par lots (batching), le mélange des données (shuffling) et le chargement parallèle (num_workers). La fonction `collate_fn` est utilisée pour regrouper correctement les données de chaque lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311c3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Classe Dataset pour charger les données depuis des fichiers .npy.\"\"\"\n",
    "    def __init__(self, directory):\n",
    "        self.directory = directory\n",
    "        # Liste les fichiers .npy dans le répertoire, triés par date de modification\n",
    "        paths = sorted(Path(self.directory).iterdir(), key=os.path.getmtime)\n",
    "        # Extrait les noms de fichiers\n",
    "        self.files = [str(p).strip().split('/')[-1] for p in paths]\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Retourne le nombre total de fichiers (échantillons) dans le dataset.\"\"\"\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Charge et retourne un échantillon de données à partir de son index.\"\"\"\n",
    "        # Construit le chemin complet du fichier\n",
    "        filepath = self.directory + self.files[idx]\n",
    "        # Gère le cas où le chemin est déjà complet (peut arriver selon la construction de self.files)\n",
    "        if len(self.files[idx]) > len(self.directory):\n",
    "             filepath = self.files[idx]\n",
    "        # Charge le fichier .npy (allow_pickle=True est nécessaire si les arrays contiennent des objets)\n",
    "        return np.load(filepath, allow_pickle=True)\n",
    "\n",
    "\n",
    "def mydataloader(dataset, batch_size, num_workers, shuffle=False):\n",
    "    \"\"\"Crée un DataLoader PyTorch pour le dataset donné.\"\"\"\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "                dataset, \n",
    "                batch_size=batch_size, # Taille du batch\n",
    "                shuffle=shuffle, # Mélanger les données à chaque époque ?\n",
    "                num_workers=num_workers, # Nombre de processus pour charger les données en parallèle\n",
    "                # collate_fn regroupe les échantillons d'un batch\n",
    "                collate_fn=lambda xs: list(zip(*xs)), \n",
    "                pin_memory=True # Accélère le transfert de données vers le GPU si possible\n",
    "                )\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a666a9",
   "metadata": {},
   "source": [
    "## Optimisation des Hyperparamètres\n",
    "\n",
    "La fonction `objective` est conçue pour être utilisée avec la bibliothèque `hyperopt`. Elle prend un dictionnaire d'hyperparamètres (`params`), configure et entraîne un modèle QDF avec ces paramètres pendant un nombre limité d'époques, puis évalue sa performance (MAE) sur l'ensemble de validation. Cette MAE est renvoyée comme la 'perte' que `hyperopt` cherchera à minimiser pour trouver la meilleure combinaison d'hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \"\"\"Fonction objectif à minimiser par hyperopt pour trouver les meilleurs hyperparamètres.\"\"\"\n",
    "    # Récupérer les hyperparamètres à tester depuis le dictionnaire 'params'\n",
    "    dim = int(params['dim'])\n",
    "    layer_functional = int(params['layer_functional'])\n",
    "    hidden_HK = int(params['hidden_HK'])\n",
    "    layer_HK = int(params['layer_HK'])\n",
    "    lr = params['lr']\n",
    "    lr_decay = params['lr_decay']\n",
    "    \n",
    "    # Créer les datasets d'entraînement et de validation\n",
    "    dataset_train = MyDataset(params[\"data\"][\"train\"])\n",
    "    dataset_val = MyDataset(params[\"data\"][\"val\"])\n",
    "    # Limiter la taille du dataset d'entraînement pour accélérer l'optimisation\n",
    "    dataset_train.files = dataset_train.files[:550]   \n",
    "    # Créer les dataloaders\n",
    "    dataloader_train = mydataloader(dataset_train, batch_size=int(params[\"batch_size\"]), num_workers=0, shuffle=True)\n",
    "    dataloader_val = mydataloader(dataset_val, batch_size=int(params[\"batch_size\"]), num_workers=0)\n",
    "    \n",
    "    # Définir le device (GPU si disponible, sinon CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Récupérer d'autres paramètres nécessaires au modèle\n",
    "    N_orbitals = params[\"N_orbitals\"]\n",
    "    N_output = params[\"N_output\"]\n",
    "    operation = params[\"operation\"]\n",
    "    # Créer le modèle QDF avec les hyperparamètres courants\n",
    "    model = QuantumDeepField(device, N_orbitals, dim, layer_functional, operation, N_output, hidden_HK, layer_HK).to(device)\n",
    "    \n",
    "    # Créer l'entraîneur et le testeur\n",
    "    trainer = Trainer(model, lr, lr_decay, step_size=params['step_size'])\n",
    "    tester = Tester(model)\n",
    "    \n",
    "    # Entraîner le modèle pendant un nombre fixe d'époques pour évaluer les hyperparamètres\n",
    "    n_epochs = 20\n",
    "    for epoch in range(n_epochs):\n",
    "        loss_E, loss_V = trainer.train(dataloader_train)\n",
    "        # Évaluer la performance sur l'ensemble de validation à chaque époque (ou à la fin)\n",
    "        MAE_val, prediction, acc_test = tester.test(dataloader_val)\n",
    "        \n",
    "    # Extraire la MAE de validation (peut être une chaîne si plusieurs sorties)\n",
    "    try:\n",
    "        mae = float(MAE_val)\n",
    "    except:\n",
    "        # Si MAE_val est une chaîne (ex: '0.1,0.2'), calculer la moyenne\n",
    "        mae = np.mean([float(m) for m in MAE_val.split(',')])\n",
    "        \n",
    "    # Afficher les paramètres testés et la MAE obtenue\n",
    "    print(\"Params:\", params, \"=> MAE:\", mae)\n",
    "    # Retourner un dictionnaire attendu par hyperopt, contenant la perte (MAE) à minimiser\n",
    "    return {'loss': mae, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a714fe06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
